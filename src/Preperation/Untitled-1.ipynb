{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including random, json, numpy, nltk, and tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess Data\n",
    "Load the intents.json file and initialize lists for words, classes, documents, and ignore_letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess Data\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load intents.json file\n",
    "with open('intents.json') as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Initialize lists\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = ['?', '!', '.', ',']\n",
    "\n",
    "# Process each intent\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# Lemmatize and sort words\n",
    "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
    "words = sorted(set(words))\n",
    "\n",
    "# Sort classes\n",
    "classes = sorted(set(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Lemmatizer Initialization\n",
    "Correct the initialization of the WordNetLemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Lemmatizer Initialization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize and sort words\n",
    "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
    "words = sorted(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and Lemmatize Words\n",
    "Tokenize the patterns, extend the words list, append to documents, and add unique classes. Lemmatize the words correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and Lemmatize Words\n",
    "\n",
    "# Initialize lists\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = ['?', '!', '.', ',']\n",
    "\n",
    "# Process each intent\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# Lemmatize and sort words\n",
    "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
    "words = sorted(set(words))\n",
    "\n",
    "# Sort classes\n",
    "classes = sorted(set(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Processed Words and Classes\n",
    "Print the processed words and classes to verify the preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are', 'Can', 'Do', 'Explain', 'Flask', 'Good', 'Goodbye', 'Have', 'Hello', 'Hey', 'Hi', 'How', 'I', 'Is', 'Leaving', 'See', 'Tell', 'What', 'Whats', 'a', 'about', 'am', 'anyone', 'are', 'bye', 'cao', 'coding', 'cya', 'day', 'development', 'do', 'feel', 'good', 'greeting', 'is', 'know', 'later', 'me', 'progamming', 'programming', 'see', 'software', 'something', 'tell', 'there', 'to', 'up', 'ya', 'you', 'your']\n",
      "['flask', 'goodbye', 'greeting', 'how', 'programming']\n"
     ]
    }
   ],
   "source": [
    "# Print Processed Words and Classes\n",
    "print(words)\n",
    "print(classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
